import os
import glob
import cv2
import json
import numpy as np
import torch
from ultralytics import YOLO

# torchreid
from torchreid.reid.utils import FeatureExtractor

# 顔検出用カスケード
FACE_CASCADE = cv2.CascadeClassifier(
    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
)



def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)


def cosine_distance(a: np.ndarray, b: np.ndarray) -> float:
    # a,b: (D,) float32
    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-12
    return float(1.0 - np.dot(a, b) / denom)


def crop_with_padding(img, x1, y1, x2, y2, pad_ratio=0.05):
    h, w = img.shape[:2]
    bw = x2 - x1
    bh = y2 - y1
    px = int(bw * pad_ratio)
    py = int(bh * pad_ratio)
    x1p = max(0, int(x1 - px))
    y1p = max(0, int(y1 - py))
    x2p = min(w - 1, int(x2 + px))
    y2p = min(h - 1, int(y2 + py))
    return img[y1p:y2p, x1p:x2p], (x1p, y1p, x2p, y2p)


def draw_person(img, x1, y1, x2, y2, text, color=(0, 255, 0)):
    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    cv2.putText(
        img,
        text,
        (x1, max(0, y1 - 8)),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.6,
        color,
        2,
        cv2.LINE_AA,
    )

def extract_face_crop(crop):
    """
    cropから顔を検出してcropする
    顔が見つかれば顔crop, なければNoneを返す
    """
    if crop.size == 0:
        return None
    
    # 顔検出
    faces = FACE_CASCADE.detectMultiScale(crop, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    
    if len(faces) == 0:
        return None
    
    # 最大の顔を選択
    face = max(faces, key=lambda f: f[2] * f[3])
    fx, fy, fw, fh = face
    
    # 顔周辺をpaddingして取得
    pad = int(max(fw, fh) * 0.2)
    fx1 = max(0, fx - pad)
    fy1 = max(0, fy - pad)
    fx2 = min(crop.shape[1], fx + fw + pad)
    fy2 = min(crop.shape[0], fh + fh + pad)
    
    face_crop = crop[fy1:fy2, fx1:fx2].copy()
    if face_crop.size == 0:
        return None
    
    return face_crop

def compare_faces(face_emb1, face_emb2, threshold=0.4):
    """
    2つの顔embeddingを比較
    距離がthreshold以下なら同じ顔と判定
    """
    distance = cosine_distance(face_emb1, face_emb2)
    return distance < threshold, distance

def box_iou(box1, box2):
    """2つのBBoxのIoU（Intersection over Union）を計算"""
    x1_min, y1_min, x1_max, y1_max = box1
    x2_min, y2_min, x2_max, y2_max = box2
    
    # 交差領域
    inter_x_min = max(x1_min, x2_min)
    inter_y_min = max(y1_min, y2_min)
    inter_x_max = min(x1_max, x2_max)
    inter_y_max = min(y1_max, y2_max)
    
    inter_w = max(0, inter_x_max - inter_x_min)
    inter_h = max(0, inter_y_max - inter_y_min)
    inter_area = inter_w * inter_h
    
    # 和領域
    area1 = (x1_max - x1_min) * (y1_max - y1_min)
    area2 = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = area1 + area2 - inter_area
    
    if union_area == 0:
        return 0
    
    return inter_area / union_area

def remove_duplicate_boxes(boxes, iou_threshold=0.3):
    """
    似ているBBoxを除外（重複排除）
    IoU > iou_threshold の場合、信頼度が低い方を削除
    """
    if len(boxes) == 0:
        return []
    
    # 信頼度でソート（降順）
    sorted_boxes = sorted(enumerate(boxes), key=lambda x: x[1][4], reverse=True)
    kept_indices = []
    
    for i, (idx1, box1) in enumerate(sorted_boxes):
        is_duplicate = False
        bbox1 = box1[:4]
        
        # 既に保持されているboxとの比較
        for kept_idx in kept_indices:
            bbox2 = boxes[kept_idx][:4]
            iou = box_iou(bbox1, bbox2)
            
            if iou > iou_threshold:
                is_duplicate = True
                break
        
        if not is_duplicate:
            kept_indices.append(idx1)
    
    # 元の順序を保持
    kept_indices.sort()
    return [boxes[i] for i in kept_indices]


class PersonDB:
    def ranked_matches(self, emb: np.ndarray):
        """
        各personの複数embの中で最小距離を取り、
        (person_id, min_distance)でソート
        """
        pairs = []
        for p in self.people:
            # embsリストの中で最近傍を取る
            min_dist = min([cosine_distance(emb, e) for e in p["embs"]])
            pairs.append((p["id"], min_dist))
        pairs.sort(key=lambda x: x[1])  # dist昇順
        return pairs
    
    def __init__(self):
        self.people = []  # list of dict {id, embs: [複数のemb], face_embs: [顔emb], n}

    def add_new(self, emb: np.ndarray, face_emb: np.ndarray = None) -> int:
        new_id = len(self.people)
        face_embs = [face_emb.copy()] if face_emb is not None else []
        self.people.append({"id": new_id, "embs": [emb.copy()], "face_embs": face_embs, "n": 1})
        return new_id

    def update(self, pid: int, emb: np.ndarray, face_emb: np.ndarray = None, T_update: float = 0.20):
        """
        距離がT_update以下の時だけembを追加
        異なる視点の特徴を保持（最多10個まで保存）
        """
        p = self.people[pid]
        # 既存のembとの最小距離を計算
        min_dist = min([cosine_distance(emb, e) for e in p["embs"]])
        
        # 十分近い時だけ追加（最多10個まで保有）
        if min_dist < T_update and len(p["embs"]) < 10:
            p["embs"].append(emb.copy())
        
        # 顔embも保存（同一ID内の顔比較用）
        if face_emb is not None:
            if len(p["face_embs"]) < 5:  # 顔embは最大5個
                p["face_embs"].append(face_emb.copy())
        
        p["n"] += 1

    def match(self, emb: np.ndarray, T_same: float, T_gap: float = 0.05, exclude_ids: set = None):
        """
        複数embの最近傍 + 2位との差で判定
        d1 < T_same かつ (d2 - d1) > T_gap の時だけマッチ
        exclude_ids: このフレームで既に使用されたIDを除外
        """
        if not self.people:
            return None, None

        pairs = self.ranked_matches(emb)
        
        if len(pairs) == 0:
            return None, None
        
        if exclude_ids is None:
            exclude_ids = set()
        
        # exclude_idsに含まれないIDで最小距離を見つける
        filtered_pairs = [(pid, dist) for pid, dist in pairs if pid not in exclude_ids]
        
        if len(filtered_pairs) == 0:
            return None, pairs[0][1]  # 全てexcludeされた場合
        
        d1 = filtered_pairs[0][1]
        d2 = filtered_pairs[1][1] if len(filtered_pairs) >= 2 else 999
        
        # 1位が十分近く、かつ2位より十分離れている
        if d1 < T_same and (d2 - d1) > T_gap:
            return filtered_pairs[0][0], d1
        
        return None, d1
    
    def verify_face(self, pid: int, face_emb: np.ndarray, face_threshold=0.4):
        """
        同一IDの既存顔embと比較して同じ顔かどうか確認
        複数枚の写真がある場合のみ顔比較を実施
        返値: (same_face: bool, min_distance: float)
        """
        p = self.people[pid]
        
        # 顔embが2枚以上ある場合のみ比較
        if len(p["face_embs"]) < 2:
            return True, 0.0  # 1枚のみなら同じ顔と判定
        
        # 既存の顔embと比較
        min_dist = 999
        for existing_face_emb in p["face_embs"]:
            dist = cosine_distance(face_emb, existing_face_emb)
            min_dist = min(min_dist, dist)
        
        same_face = min_dist < face_threshold
        return same_face, min_dist


def main():
    in_dir = "images"
    out_dir = "out"
    crops_dir = os.path.join(out_dir, "crops")
    ensure_dir(out_dir)
    ensure_dir(crops_dir)
    
    # JSONL出力ファイル（capture_and_merge.pyで監視される）
    detections_file = "detections.jsonl"
    detection_records = []

    # 1) Person detection
    det_model = YOLO("yolov8m.pt")  # nより強い, 検出精度重視
    PERSON_CLASS_ID = 0

    # 2) ReID feature extractor
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ReID device = {device}")

    # OSNetが軽くて強いです.
    extractor = FeatureExtractor(
        model_name="osnet_x1_0",
        model_path="",  # empty = pretrained
        device=device,
    )

    exts = ["*.jpg", "*.jpeg", "*.png", "*.webp"]
    paths = []
    for e in exts:
        paths += glob.glob(os.path.join(in_dir, e))
    paths.sort()

    if not paths:
        print("images/ に画像が見つかりません.")
        return

    # ユーザー入力: 閾値のみ
    print(f"\n合計 {len(paths)} 枚の画像が見つかりました")
    print("ファイル: " + ", ".join([os.path.basename(p) for p in paths]))
    
    threshold_input = input("\n閾値 (デフォルト: 0.25, 小さいほど厳しい): ").strip()
    T_same = float(threshold_input) if threshold_input else 0.25
    
    # 全ファイルを処理
    start_idx = 1
    end_idx = len(paths)
    selected_paths = paths
    print(f"\n選択ファイル範囲: {start_idx} ~ {end_idx} ({len(selected_paths)}枚)")
    for i, p in enumerate(selected_paths, start_idx):
        print(f"  {i}: {os.path.basename(p)}")
    print(f"閾値: {T_same}\n")

    if not paths:
        print("images/ に画像が見つかりません.")
        return

    db = PersonDB()

    for img_idx, path in enumerate(selected_paths):
        img = cv2.imread(path)
        if img is None:
            print(f"読み込み失敗: {path}")
            continue

        results = det_model.predict(img, conf=0.6, verbose=False)  # 信頼度を上げて誤検出を削減（椅子などの家具誤検出を防止）
        r = results[0]

        dets = []
        if r.boxes is not None and len(r.boxes) > 0:
            for b in r.boxes:
                cls = int(b.cls.item())
                if cls != PERSON_CLASS_ID:
                    continue
                x1, y1, x2, y2 = b.xyxy[0].tolist()
                conf = float(b.conf.item())
                dets.append((x1, y1, x2, y2, conf))
        
        # 重複するBBoxを除外（IoUで似ている検出は同じものとしてカウント）
        dets = remove_duplicate_boxes(dets, iou_threshold=0.3)

        vis = img.copy()
        used_ids = set()
        # 各人物のcropを作って, ReID特徴を抽出して, DBと照合
        for p_idx, (x1, y1, x2, y2, conf) in enumerate(dets):
            crop, (cx1, cy1, cx2, cy2) = crop_with_padding(img, x1, y1, x2, y2, pad_ratio=0.05)
            if crop.size == 0:
                continue

            # extractorはBGR/np.arrayも受けますが, 内部のPIL変換されます.
            emb_t = extractor([crop])  # torch tensor (1, D)
            emb = emb_t[0].detach().cpu().numpy().astype(np.float32)
            
            # 顔embを抽出
            face_crop = extract_face_crop(crop)
            face_emb = None
            if face_crop is not None:
                face_emb_t = extractor([face_crop])
                face_emb = face_emb_t[0].detach().cpu().numpy().astype(np.float32)

            # 新しいmatch()を使用: 1位 + 2位との差で判定
            # used_idsで同一フレーム内の重複を防ぐ
            pid, best_dist = db.match(emb, T_same, T_gap=0.05, exclude_ids=used_ids)

            if pid is None:
                pid = db.add_new(emb, face_emb=face_emb)
                matched = "NEW"
            else:
                # 顔embが取得できた場合、同一IDの既存顔と比較
                face_valid = True
                face_dist = 0.0
                if face_emb is not None:
                    face_valid, face_dist = db.verify_face(pid, face_emb, face_threshold=0.4)
                
                if not face_valid:
                    # 顔が異なる場合は別IDを作成
                    pid = db.add_new(emb, face_emb=face_emb)
                    matched = "NEW_FACE"
                else:
                    # 距離が近い時だけembを追加（複数視点を保持）
                    db.update(pid, emb, face_emb=face_emb, T_update=0.20)
                    matched = "OK"
            
            # 同一フレーム内でこのIDが使用済みになったことを記録
            used_ids.add(pid)

            show_dist = best_dist if best_dist is not None else 0.0


            # crop保存
            id_dir = os.path.join(crops_dir, f"ID_{pid:03d}")
            ensure_dir(id_dir)
            crop_out = os.path.join(id_dir, f"img{img_idx:03d}_p{p_idx:02d}.jpg")
            cv2.imwrite(crop_out, crop)

            # 可視化
            txt = f"ID {pid:03d} {matched} d={show_dist:.3f}"
            draw_person(vis, cx1, cy1, cx2, cy2, txt)

        base = os.path.splitext(os.path.basename(path))[0]
        out_path = os.path.join(out_dir, f"{img_idx:03d}_{base}_detid.jpg")
        cv2.imwrite(out_path, vis)
        print(f"[{img_idx+1}/{len(selected_paths)}] dets={len(dets)} -> {out_path}")
        
        # JSONL形式で検出結果を記録（capture_and_merge.pyで使用）
        boxes_with_ids = []
        for p_idx, (x1, y1, x2, y2, conf) in enumerate(dets):
            crop, (cx1, cy1, cx2, cy2) = crop_with_padding(img, x1, y1, x2, y2, pad_ratio=0.05)
            if crop.size == 0:
                continue
            emb_t = extractor([crop])
            emb = emb_t[0].detach().cpu().numpy().astype(np.float32)
            pid, best_dist = db.match(emb, T_same, T_gap=0.05, exclude_ids=used_ids)
            
            boxes_with_ids.append({
                "box": [x1, y1, x2, y2],
                "person_id": pid if pid is not None else -1,
                "confidence": conf
            })
        
        detection_records.append({
            "frame_id": img_idx + 1,
            "filename": base,
            "boxes": [[b["box"][0], b["box"][1], b["box"][2], b["box"][3]] for b in boxes_with_ids]
        })

    print(f"Unique persons = {len(db.people)}")
    print(f"Threshold T_same = {T_same}")
    
    # 検出結果をJSONLに保存
    with open(detections_file, 'w') as f:
        for record in detection_records:
            f.write(json.dumps(record) + "\n")
    print(f"\nDetections saved to {detections_file}")


if __name__ == "__main__":
    main()
